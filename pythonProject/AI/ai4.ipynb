{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython\n",
    "import mindspore.nn as nn\n",
    "from mindspore import dtype as mstype\n",
    "import mindspore.dataset as ds\n",
    "import mindspore.dataset.vision.c_transforms as C\n",
    "import mindspore.dataset.transforms.c_transforms as C2\n",
    "from mindspore import context\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "context.set_context(mode=context.GRAPH_MODE, device_target=\"CPU\")\n",
    "def create_dataset(data_home, repeat_num=1, batch_size=32, do_train=True, device_target=\"CPU\"):\n",
    "    \"\"\"\n",
    "    create data for next use such as training or inferring\n",
    "    \"\"\"\n",
    "\n",
    "    cifar_ds = ds.Cifar10Dataset(data_home,num_parallel_workers=8, shuffle=True)\n",
    "\n",
    "    c_trans = []\n",
    "    if do_train:\n",
    "        c_trans += [\n",
    "            C.RandomCrop((32, 32), (4, 4, 4, 4)),\n",
    "            C.RandomHorizontalFlip(prob=0.5)\n",
    "        ]\n",
    "\n",
    "    c_trans += [\n",
    "        C.Resize((224, 224)),\n",
    "        C.Rescale(1.0 / 255.0, 0.0),\n",
    "        C.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010]),\n",
    "        C.HWC2CHW()\n",
    "    ]\n",
    "\n",
    "    type_cast_op = C2.TypeCast(mstype.int32)\n",
    "\n",
    "    cifar_ds = cifar_ds.map(operations=type_cast_op, input_columns=\"label\", num_parallel_workers=8)\n",
    "    cifar_ds = cifar_ds.map(operations=c_trans, input_columns=\"image\", num_parallel_workers=8)\n",
    "\n",
    "    cifar_ds = cifar_ds.batch(batch_size, drop_remainder=True)\n",
    "    cifar_ds = cifar_ds.repeat(repeat_num)\n",
    "\n",
    "    return cifar_ds\n",
    "\n",
    "\n",
    "ds_train_path = \"./cifar10/train/\"\n",
    "dataset_show = create_dataset(ds_train_path)\n",
    "with open(ds_train_path+\"batches.meta.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    all_name = [name.replace(\"\\n\",\"\") for name in f.readlines()]\n",
    "\n",
    "iterator_show= dataset_show.create_dict_iterator()\n",
    "dict_data = next(iterator_show)\n",
    "images = dict_data[\"image\"].asnumpy()\n",
    "labels = dict_data[\"label\"].asnumpy()\n",
    "count = 1\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "for i in images:\n",
    "    plt.subplot(4, 8, count)\n",
    "    # Images[0].shape is (3,224,224).We need transpose as (224,224,3) for using in plt.show().\n",
    "    picture_show = np.transpose(i,(1,2,0))\n",
    "    picture_show = picture_show/np.amax(picture_show)\n",
    "    picture_show = np.clip(picture_show, 0, 1)\n",
    "    plt.title(all_name[labels[count-1]])\n",
    "    picture_show = np.array(picture_show,np.float32)\n",
    "    plt.imshow(picture_show)\n",
    "    count += 1\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "print(\"The dataset size is:\", dataset_show.get_dataset_size())\n",
    "print(\"The batch tensor is:\",images.shape)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "from resnet import resnet50\n",
    "\n",
    "net = resnet50(batch_size=32, num_classes=10)\n",
    "\n",
    "import mindspore.nn as nn\n",
    "from mindspore.nn import SoftmaxCrossEntropyWithLogits\n",
    "\n",
    "ls = SoftmaxCrossEntropyWithLogits(sparse=True, reduction=\"mean\")\n",
    "opt = nn.Momentum(filter(lambda x: x.requires_grad, net.get_parameters()), 0.01, 0.9)\n",
    "\n",
    "\n",
    "\n",
    "from mindspore.train.callback import ModelCheckpoint, CheckpointConfig, LossMonitor\n",
    "from mindspore import load_checkpoint, load_param_into_net\n",
    "import os\n",
    "from mindspore import Model\n",
    "\n",
    "model = Model(net, loss_fn=ls, optimizer=opt, metrics={'acc'})\n",
    "# As for train, users could use model.train\n",
    "\n",
    "epoch_size = 10\n",
    "ds_train_path = \"./cifar10/train/\"\n",
    "model_path = \"./models/ckpt/mindspore_vision_application/\"\n",
    "os.system('rm -f {0}*.ckpt {0}*.meta {0}*.pb'.format(model_path))\n",
    "\n",
    "dataset = create_dataset(ds_train_path )\n",
    "batch_num = dataset.get_dataset_size()\n",
    "config_ck = CheckpointConfig(save_checkpoint_steps=batch_num, keep_checkpoint_max=35)\n",
    "ckpoint_cb = ModelCheckpoint(prefix=\"train_resnet_cifar10\", directory=model_path, config=config_ck)\n",
    "loss_cb = LossMonitor(142)\n",
    "model.train(epoch_size, dataset, callbacks=[ckpoint_cb, loss_cb])\n",
    "\n",
    "# As for evaluation, users could use model.eval\n",
    "ds_eval_path = \"./cifar10/test/\"\n",
    "eval_dataset = create_dataset(ds_eval_path, do_train=False)\n",
    "res = model.eval(eval_dataset)\n",
    "\n",
    "def writeCsv(File):\n",
    "    row = [File]\n",
    "    out = open(\"data/test.csv\", \"a\", newline=\"\")\n",
    "    csv_writer = csv.writer(out, dialect=\"excel\")\n",
    "    csv_writer.writerow(row)\n",
    "\n",
    "writeCsv(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "import time\n",
    "for i in trange(10): \n",
    "    time.sleep(1)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5179d32cf6ec497baf3f8a3ef987cc77c5d2dc691fdde20a56316522f61a7323"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
