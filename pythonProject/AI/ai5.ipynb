{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:[3.45638663]\n",
      "Pred:[0 0 0 0 0 0 0 1]\n",
      "True:[0 1 0 0 0 1 0 1]\n",
      "9 + 60 = 1\n",
      "------------\n",
      "Error:[3.63389116]\n",
      "Pred:[1 1 1 1 1 1 1 1]\n",
      "True:[0 0 1 1 1 1 1 1]\n",
      "28 + 35 = 255\n",
      "------------\n",
      "Error:[3.91366595]\n",
      "Pred:[0 1 0 0 1 0 0 0]\n",
      "True:[1 0 1 0 0 0 0 0]\n",
      "116 + 44 = 72\n",
      "------------\n",
      "Error:[3.72191702]\n",
      "Pred:[1 1 0 1 1 1 1 1]\n",
      "True:[0 1 0 0 1 1 0 1]\n",
      "4 + 73 = 223\n",
      "------------\n",
      "Error:[3.5852713]\n",
      "Pred:[0 0 0 0 1 0 0 0]\n",
      "True:[0 1 0 1 0 0 1 0]\n",
      "71 + 11 = 8\n",
      "------------\n",
      "Error:[2.53352328]\n",
      "Pred:[1 0 1 0 0 0 1 0]\n",
      "True:[1 1 0 0 0 0 1 0]\n",
      "81 + 113 = 162\n",
      "------------\n",
      "Error:[0.57691441]\n",
      "Pred:[0 1 0 1 0 0 0 1]\n",
      "True:[0 1 0 1 0 0 0 1]\n",
      "81 + 0 = 81\n",
      "------------\n",
      "Error:[1.42589952]\n",
      "Pred:[1 0 0 0 0 0 0 1]\n",
      "True:[1 0 0 0 0 0 0 1]\n",
      "4 + 125 = 129\n",
      "------------\n",
      "Error:[0.47477457]\n",
      "Pred:[0 0 1 1 1 0 0 0]\n",
      "True:[0 0 1 1 1 0 0 0]\n",
      "39 + 17 = 56\n",
      "------------\n",
      "Error:[0.21595037]\n",
      "Pred:[0 0 0 0 1 1 1 0]\n",
      "True:[0 0 0 0 1 1 1 0]\n",
      "11 + 3 = 14\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "import copy, numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "# compute sigmoid nonlinearity\n",
    "def sigmoid(x):\n",
    "    output = 1/(1+np.exp(-x))\n",
    "    return output\n",
    "\n",
    "# convert output of sigmoid function to its derivative\n",
    "def sigmoid_output_to_derivative(output):\n",
    "    return output*(1-output)\n",
    "\n",
    "\n",
    "# training dataset generation\n",
    "int2binary = {}\n",
    "binary_dim = 8\n",
    "\n",
    "largest_number = pow(2,binary_dim)\n",
    "binary = np.unpackbits(\n",
    "    np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n",
    "for i in range(largest_number):\n",
    "    int2binary[i] = binary[i]\n",
    "\n",
    "\n",
    "# input variables\n",
    "alpha = 0.1\n",
    "input_dim = 2\n",
    "hidden_dim = 16\n",
    "output_dim = 1\n",
    "\n",
    "\n",
    "# initialize neural network weights\n",
    "synapse_0 = 2*np.random.random((input_dim,hidden_dim)) - 1\n",
    "synapse_1 = 2*np.random.random((hidden_dim,output_dim)) - 1\n",
    "synapse_h = 2*np.random.random((hidden_dim,hidden_dim)) - 1\n",
    "\n",
    "synapse_0_update = np.zeros_like(synapse_0)\n",
    "synapse_1_update = np.zeros_like(synapse_1)\n",
    "synapse_h_update = np.zeros_like(synapse_h)\n",
    "\n",
    "# training logic\n",
    "for j in range(10000):\n",
    "    \n",
    "    # generate a simple addition problem (a + b = c)\n",
    "    a_int = np.random.randint(largest_number/2) # int version\n",
    "    a = int2binary[a_int] # binary encoding\n",
    "\n",
    "    b_int = np.random.randint(largest_number/2) # int version\n",
    "    b = int2binary[b_int] # binary encoding\n",
    "\n",
    "    # true answer\n",
    "    c_int = a_int + b_int\n",
    "    c = int2binary[c_int]\n",
    "    \n",
    "    # where we'll store our best guess (binary encoded)\n",
    "    d = np.zeros_like(c)\n",
    "\n",
    "    overallError = 0\n",
    "    \n",
    "    layer_2_deltas = list()\n",
    "    layer_1_values = list()\n",
    "    layer_1_values.append(np.zeros(hidden_dim))\n",
    "    \n",
    "    # moving along the positions in the binary encoding\n",
    "    for position in range(binary_dim):\n",
    "        \n",
    "        # generate input and output\n",
    "        X = np.array([[a[binary_dim - position - 1],b[binary_dim - position - 1]]])\n",
    "        y = np.array([[c[binary_dim - position - 1]]]).T\n",
    "\n",
    "        # hidden layer (input ~+ prev_hidden)\n",
    "        layer_1 = sigmoid(np.dot(X,synapse_0) + np.dot(layer_1_values[-1],synapse_h))\n",
    "\n",
    "        # output layer (new binary representation)\n",
    "        layer_2 = sigmoid(np.dot(layer_1,synapse_1))\n",
    "\n",
    "        # did we miss?... if so, by how much?\n",
    "        layer_2_error = y - layer_2\n",
    "        layer_2_deltas.append((layer_2_error)*sigmoid_output_to_derivative(layer_2))\n",
    "        overallError += np.abs(layer_2_error[0])\n",
    "    \n",
    "        # decode estimate so we can print it out\n",
    "        d[binary_dim - position - 1] = np.round(layer_2[0][0])\n",
    "        \n",
    "        # store hidden layer so we can use it in the next timestep\n",
    "        layer_1_values.append(copy.deepcopy(layer_1))\n",
    "    \n",
    "    future_layer_1_delta = np.zeros(hidden_dim)\n",
    "    \n",
    "    for position in range(binary_dim):\n",
    "        \n",
    "        X = np.array([[a[position],b[position]]])\n",
    "        layer_1 = layer_1_values[-position-1]\n",
    "        prev_layer_1 = layer_1_values[-position-2]\n",
    "        \n",
    "        # error at output layer\n",
    "        layer_2_delta = layer_2_deltas[-position-1]\n",
    "        # error at hidden layer\n",
    "        layer_1_delta = (future_layer_1_delta.dot(synapse_h.T) + layer_2_delta.dot(synapse_1.T)) * sigmoid_output_to_derivative(layer_1)\n",
    "\n",
    "        # let's update all our weights so we can try again\n",
    "        synapse_1_update += np.atleast_2d(layer_1).T.dot(layer_2_delta)\n",
    "        synapse_h_update += np.atleast_2d(prev_layer_1).T.dot(layer_1_delta)\n",
    "        synapse_0_update += X.T.dot(layer_1_delta)\n",
    "        \n",
    "        future_layer_1_delta = layer_1_delta\n",
    "    \n",
    "\n",
    "    synapse_0 += synapse_0_update * alpha\n",
    "    synapse_1 += synapse_1_update * alpha\n",
    "    synapse_h += synapse_h_update * alpha    \n",
    "\n",
    "    synapse_0_update *= 0\n",
    "    synapse_1_update *= 0\n",
    "    synapse_h_update *= 0\n",
    "    \n",
    "    # print out progress\n",
    "    if(j % 1000 == 0):\n",
    "        print (\"Error:\" + str(overallError))\n",
    "        print (\"Pred:\" + str(d))\n",
    "        print (\"True:\" + str(c))\n",
    "        out = 0\n",
    "        for index,x in enumerate(reversed(d)):\n",
    "            out += x*pow(2,index)\n",
    "        print (str(a_int) + \" + \" + str(b_int) + \" = \" + str(out))\n",
    "        print (\"------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "\n",
    "if gpus:\n",
    "    gpu0 = gpus[0] #如果有多个GPU，仅使用第0个GPU\n",
    "    tf.config.experimental.set_memory_growth(gpu0, True) #设置GPU显存用量按需使用\n",
    "    tf.config.set_visible_devices([gpu0],\"GPU\")\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()\n",
    "# 将像素的值标准化至0到1的区间内。\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "\n",
    "train_images.shape,test_images.shape,train_labels.shape,test_labels.shape\n",
    "\"\"\"\n",
    "输出：((60000, 28, 28), (10000, 28, 28), (60000,), (10000,))\n",
    "\"\"\"\n",
    "plt.figure(figsize=(20,10))\n",
    "for i in range(20):\n",
    "    plt.subplot(5,10,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(train_labels[i])\n",
    "plt.show()\n",
    "#调整数据到我们需要的格式\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "\n",
    "train_images.shape,test_images.shape,train_labels.shape,test_labels.shape\n",
    "\"\"\"\n",
    "输出：((60000, 28, 28, 1), (10000, 28, 28, 1), (60000,), (10000,))\n",
    "\"\"\"\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),#卷积层1，卷积核3*3\n",
    "    layers.MaxPooling2D((2, 2)),                   #池化层1，2*2采样\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),  #卷积层2，卷积核3*3\n",
    "    layers.MaxPooling2D((2, 2)),                   #池化层2，2*2采样\n",
    "    \n",
    "    layers.Flatten(),                              #Flatten层，连接卷积层与全连接层\n",
    "    layers.Dense(64, activation='relu'),\t\t   #全连接层，特征进一步提取\n",
    "    layers.Dense(10)                               #输出层，输出预期结果\n",
    "])\n",
    "# 打印网络结构\n",
    "model.summary()\n",
    "model.compile(optimizer='adam',\n",
    "            loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            metrics=['accuracy'])\n",
    "history = model.fit(train_images, train_labels, epochs=10, \n",
    "                    validation_data=(test_images, test_labels))\n",
    "plt.imshow(test_images[1])\n",
    "pre = model.predict(test_images) # 对所有测试图片进行预测\n",
    "pre[1] # 输出第一张图片的预测结果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入数据\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pickle\n",
    "import time\n",
    "\n",
    "\n",
    "def unpickle(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        d = pickle.load(f, encoding='latin1')\n",
    "        return d\n",
    "\n",
    "def onehot(labels):\n",
    "    '''\n",
    "    one-hot 编码\n",
    "    '''\n",
    "    n_sample = len(labels)\n",
    "    n_class = max(labels) + 1\n",
    "    onehot_labels = np.zeros((n_sample, n_class))\n",
    "    onehot_labels[np.arange(n_sample), labels] = 1\n",
    "    return onehot_labels\n",
    "\n",
    "# 训练数据集\n",
    "data1 = unpickle('data_batch_1')\n",
    "data2 = unpickle('data_batch_2')\n",
    "data3 = unpickle('data_batch_3')\n",
    "data4 = unpickle('data_batch_4')\n",
    "data5 = unpickle('data_batch_5')\n",
    "X_train = np.concatenate((data1['data'], data2['data'], data3['data'], data4['data'], data5['data']), axis=0)\n",
    "y_train = np.concatenate((data1['labels'], data2['labels'], data3['labels'], data4['labels'], data5['labels']), axis=0)\n",
    "y_train = onehot(y_train)\n",
    "\n",
    "# 测试数据集\n",
    "test = unpickle('test_batch')\n",
    "X_test = test['data'][:5000, :]\n",
    "y_test = onehot(test['labels'])[:5000, :]\n",
    "\n",
    "print('Training dataset shape:', X_train.shape)\n",
    "print('Training labels shape:', y_train.shape)\n",
    "print('Testing dataset shape:', X_test.shape)\n",
    "print('Testing labels shape:', y_test.shape)\n",
    "\n",
    "# 模型参数\n",
    "learning_rate = 1e-3\n",
    "train_iters = 200\n",
    "batch_size = 50\n",
    "\n",
    "\n",
    "# 构建模型\n",
    "x = tf.compat.v1.placeholder(tf.float32, [None, 3072])\n",
    "\n",
    "# 成本函数\n",
    "y = tf.compat.v1.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "W_conv = {\n",
    "    'conv1': tf.Variable(tf.compat.v1.truncated_normal([5, 5, 3, 32], stddev=0.0001)),\n",
    "    'conv2': tf.Variable(tf.compat.v1.truncated_normal([5, 5, 32, 64],stddev=0.01)),\n",
    "    'fc3': tf.Variable(tf.compat.v1.truncated_normal([8*8*64, 384], stddev=0.1)),\n",
    "    'fc4': tf.Variable(tf.compat.v1.truncated_normal([384, 192], stddev=0.1)),\n",
    "    'fc5': tf.Variable(tf.compat.v1.truncated_normal([192, 10], stddev=0.1))\n",
    "}\n",
    "b_conv = {\n",
    "    'conv1': tf.Variable(tf.constant(0.0, dtype=tf.float32, shape=[32])),\n",
    "    'conv2': tf.Variable(tf.constant(0.1, dtype=tf.float32, shape=[64])),\n",
    "    'fc3': tf.Variable(tf.constant(0.1, dtype=tf.float32, shape=[384])),\n",
    "    'fc4': tf.Variable(tf.constant(0.1, dtype=tf.float32, shape=[192])),\n",
    "    'fc5': tf.Variable(tf.constant(0.0, dtype=tf.float32, shape=[10]))\n",
    "}\n",
    "\n",
    "X_image = tf.reshape(x, [-1, 32, 32, 3])\n",
    "# 第一层卷积\n",
    "conv1 = tf.nn.conv2d(X_image, W_conv['conv1'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "conv1 = tf.nn.bias_add(conv1, b_conv['conv1'])\n",
    "conv1 = tf.nn.relu(conv1)\n",
    "\n",
    "# 第二层卷积\n",
    "conv2 = tf.nn.conv2d(X_image, W_conv['conv2'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "conv2 = tf.nn.bias_add(conv1, b_conv['conv2'])\n",
    "conv2 = tf.nn.relu(conv1)\n",
    "# 第一层池化\n",
    "pool1 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "# 第一LRN层，Local Response Normalization\n",
    "norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001/9.0, beta=0.75)\n",
    "# 第三层卷积\n",
    "conv3 = tf.nn.conv2d(norm1, W_conv['conv3'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "conv3 = tf.nn.bias_add(conv3, b_conv['conv3'])\n",
    "conv3 = tf.nn.relu(conv3)\n",
    "# 第四层卷积\n",
    "conv4 = tf.nn.conv2d(X_image, W_conv['conv4'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "conv4 = tf.nn.bias_add(conv1, b_conv['conv4'])\n",
    "conv4 = tf.nn.relu(conv1)\n",
    "# 第二层LRN，Local Response Normalization\n",
    "norm2 = tf.nn.lrn(conv4, 4, bias=1.0, alpha=0.001/9.0, beta=0.75)\n",
    "# 第二层池化\n",
    "pool2 = tf.nn.max_pool(norm2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "reshape = tf.reshape(pool2, [-1, 8*8*64])\n",
    "\n",
    "# 第五层卷积\n",
    "conv5 = tf.nn.conv2d(X_image, W_conv['conv5'], strides=[1, 1, 1, 1], padding='SAME')\n",
    "conv5 = tf.nn.bias_add(conv1, b_conv['conv5'])\n",
    "conv5 = tf.nn.relu(conv1)\n",
    "\n",
    "#第六层 全连接\n",
    "fc6 = tf.add(tf.matmul(reshape, W_conv['fc6']), b_conv['fc6'])\n",
    "fc6 = tf.nn.relu(fc6)\n",
    "# 第三LRN层，Local Response Normalization\n",
    "norm3 = tf.nn.lrn(pool2, 4, bias=1.0, alpha=0.001/9.0, beta=0.75)\n",
    "\n",
    "#第七层全连接层\n",
    "fc7 = tf.add(tf.matmul(fc6, W_conv['fc7']), b_conv['fc7'])\n",
    "fc7 = tf.nn.relu(fc6)\n",
    "\n",
    "\n",
    "# 第八层全连接层\n",
    "fc8 = tf.nn.softmax(tf.add(tf.matmul(fc7, W_conv['fc8']), b_conv['fc8']))\n",
    "\n",
    "# 定义损失\n",
    "loss = tf.reduce_mean(tf.compat.v1.nn.softmax_cross_entropy_with_logits_v2(logits=fc8, labels=y))\n",
    "optimizer = tf.compat.v1.train.GradientDescentOptimizer(1e-3).minimize(loss)\n",
    "#optimizer = tf.train.AdamOptimizer(1e-3).minimize(loss)\n",
    "\n",
    "# 评估模型\n",
    "correct_prediction = tf.equal(tf.argmax(fc8, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "init = tf.compat.v1.global_variables_initializer()\n",
    "\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    sess.run(init)\n",
    "    a = []\n",
    "    total_batch = int(X_train.shape[0] / batch_size)\n",
    "    start_time = time.time()\n",
    "    for i in range(train_iters):\n",
    "        for batch in range(total_batch):\n",
    "            batch_x = X_train[batch*batch_size : (batch+1)*batch_size, :]\n",
    "            batch_y = y_train[batch*batch_size : (batch+1)*batch_size, :]\n",
    "            sess.run(optimizer, feed_dict={x: batch_x, y: batch_y})\n",
    "        acc = accuracy.eval(session=sess, feed_dict={x: batch_x, y: batch_y})\n",
    "        a.append(acc)\n",
    "        end_time = time.time()\n",
    "        print(\"step = %d, train_acc = %g, time = %g\"%(i, acc, (end_time - start_time)))\n",
    "        start_time = end_time\n",
    "    print(\"Optimization Finished!\")\n",
    "    # Test\n",
    "    test_acc = accuracy.eval(session=sess, feed_dict={x: X_test, y: y_test})\n",
    "    print(\"Testing Accuracy:\", test_acc)\n",
    "    plt.plot(a)\n",
    "    plt.xlabel('Iter')\n",
    "    plt.ylabel('Acc')\n",
    "    plt.title('lr = %f, ti = %d, bs = %d, acc = %f' % (learning_rate, train_iters, batch_size, test_acc))\n",
    "    plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3cd6af004486e18d8cd1b1dc71eb6e14b35da0a003c4531af785de1b844902cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
